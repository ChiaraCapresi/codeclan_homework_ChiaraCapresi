---
title: "Decision Trees"
output: html_notebook
---


```{r}
library(tidyverse)
library(rpart)
library(rpart.plot)
```

```{r}
thrones <- read_csv("data/character_data_S01-S08.csv")


head(thrones)


thrones <- thrones %>% 
  rename_with(.cols = starts_with("dth"), .fn = ~str_replace(.x, "dth", "met_dog"))

```


## Data cleaning

Don't need to scale the data due to Boolean operations at nodes

Do still need to remove NAs and do some necessary variable reduction

Do need to do some variable wrangling to ensure all columns are numerical or factor.

**Going to use our dataset to build a decision three predicting whether a character will meet a dog by the end of the series**

- can remove columns directly connected to dog meeting (except for the flag)
- can remove totally irrelevant columns such as name



```{r}
clean_thrones <- thrones %>% 
# Only keep variables of interest
  select(c(sex, religion, occupation, social_status, allegiance_last, allegiance_switched, met_dog_flag, featured_episode_count, prominence)) %>% 
# Convert to factor level
    mutate(sex = factor(sex, levels = c(1, 2, 9), labels = c("Male", "Female", "Unknown")),
    religion = factor(religion, levels = c(0, 1, 3, 4, 5, 6, 7, 9), labels = c("Great Stallion", "Lord of Light", "Faith of the Seven", "Old Gods", "Drowned God", "Many Faced God", "Other", "Unknown")),
    occupation = factor(occupation, levels = c(1, 2, 9), labels = c("Silk-collar", "Leather-collar", "Unknown")),
    social_status = factor(social_status, levels = c(1, 2, 9), labels = c("Highborn", "Lowborn", "Unknown")),
    allegiance_last = factor(allegiance_last, levels = c(1, 2, 3, 4, 5, 6, 7, 8, 9), labels = c("Stark", "Targaryen", "Night's Watch", "Lannister", "Greyjoy", "Bolton", "Frey", "Other", "Unknown")),
    allegiance_switched = factor(allegiance_switched, levels = c(1, 2, 9), labels = c("No", "Yes", "Unknown")),
    met_dog_flag = factor(met_dog_flag, levels = c(0, 1), labels = c( "Met a dog", "Didn't meet a dog"))) %>%
# Remove NAs 
 drop_na()


```


```{r}
clean_thrones 
```

## Train/test split

80% training, 20% saved for testing


```{r}
set.seed(19)

#how many rows in total

n_data <- nrow(clean_thrones)

#create a test sample index
test_index <- sample(1:n_data, size = n_data*0.2)

# split the data
thrones_test <- slice(clean_thrones, test_index)
thrones_train <- slice(clean_thrones, -test_index)

```


We can check that our test and training sets have similar proportions of characters who met a dog


```{r}
thrones_test %>% 
  janitor::tabyl(met_dog_flag)
```
```{r}
thrones_train %>% 
  janitor::tabyl(met_dog_flag)
```

Proportions are similar between test and train sets.


## Build a tree model based on our training data


`rpart` package/function - stands for recursive patitioning and regression trees

- pass it a target variable (`met_a_dog_flag`) and dataset to use (`thrones_train`)
- then pass it the variable type we are looking at
  - `class` for categorical, `anove` for continuous
  
  
  
```{r}
# 1. make tree model

thrones_fit <- rpart(
  formula = met_dog_flag ~., #include all variables
  data = thrones_train,
  method = "class" # for a categorical predictor
)

# 2. plot tree model

rpart.plot(thrones_fit,
           yesno = 2, # this writes yes/no at all splits
           type = 2, #dictates where our conditions lie at each node
           fallen.leaves = TRUE, # TRUE means leaves all alligned at bottom
           faclen = 6, #length of factor names
           digits = 2, # how many decimal places is prob reported in
           split.border.col = 1 #colour 
           )

```


Each node tell us:
 - predicted met dog or not at the top
 (root node shows us prediction before any factors taken into account - all-data-together prediction)
 - probability of a met_dog result
 - % data points that pass through this node
 - colour is related to classification and strangth of probability
   - blues for P's < 0.5, greens for P's > 0.5


Can plot with counts instead of probablilities


```{r}
rpart.plot(thrones_fit,
           yesno = 2,
           type = 2,
           fallen.leaves = TRUE,
           faclen = 6,
           digits = 2,
           extra = 101 # displays the number and % of observations in the node
           )
```


Each node:

didn't meet a dog N on the left, met a dog N on the right

Conditional probabilities!
- prob(met_dog) **given** the filtering conditions applied up to that node.

We can see the rules it has used to make the tree if we type:


```{r}
rpart.rules(thrones_fit, cover = TRUE)
```

First column gives us 'met_dog' probability (as seen on first tree plot), then rules to get to that node, then the final column gives us the % of observations following these rules.

i.e. each row is explaining which (and how) data ends up at a given leaf

## Use trained model to create predictions on the test dataset


```{r}
library(modelr)
#add the predictions

thrones_test_pred <- thrones_test %>% 
  add_predictions(thrones_fit, type = "class")
```


Let's look at our predictions, using the most informative variables:

```{r}
thrones_test_pred %>% 
  select(prominence, religion, allegiance_last, featured_episode_count, met_dog_flag, pred)
```





## Checking model performance

We can create a confusion matrix again to check performance

```{r}
library(yardstick)
```


```{r}
conf_mat <- thrones_test_pred %>% 
  conf_mat(truth = met_dog_flag, estimate = pred)

conf_mat
```

Accurate decision tree = higher main diagonal values (top left - bottom right).

Can calculate prob of prediction being correct
i.e. how many times model was right / total prediction

```{r}
accuracy <- thrones_test_pred %>% 
  accuracy(truth = met_dog_flag, estimate = pred)

accuracy
```

`.estimate` shows the probability of correctly predicting whether a character in the test set meta  dog or not in the show.

Sensitivity (true positive rate), i.e. prob of predicting met_dog when this is the case


```{r}
sensitivity <- thrones_test_pred %>% 
  sensitivity(truth = met_dog_flag, estimate = pred)

sensitivity
```


Specificity (true negative rate) i.e. prob of predicting 'didn't meet a dog' when this is true.



```{r}
specificity <- thrones_test_pred %>% 
  specificity(truth = met_dog_flag, estimate = pred)

specificity
```

Advantages of decision trees
- intuitive and easy to explain
- closely mirror human decision making compared to regression
- nice, graphical, display
- easily handle qualitative/categorical predictors without the need for dummy variables
- not sensitive to scale of variables

Disadvantages:
- trees generally do not have the same level of predictive accuracy as other approaches
- a small change in the data can cause a large change in the final estimated tree.


## Random forests

Collection of decision trees

each tree: instead of serching for the most important features from all available ones, it searches for the best feature among a random subset of features.

Generally considered more pawerful and more stable
e.g. if 50 of your trees predict an outcome as true and 10 predict false --> overall prediction of the forest will be true for that datapoint

```{r}
library(ranger)
```

```{r}
rf_classifier <- ranger(
  formula = met_dog_flag ~.,
  data = thrones_train,
  importance = "impurity",
  num.trees = 1000, #num of trees in the forest
  mtry = 2, # num variables to consider at each node
  min.node.size = 5 #minimum num of datapoints allowed at a node
)

rf_classifier
```


Let's check importance of classifiers

```{r}
importance(rf_classifier)
```


Can see that prominence is the most important variable in our decision trees

```{r}
#ranger doesn't work with add_predictions() so we have to use some dplyr

thrones_test_pred_rf <- thrones_test %>% 
  mutate(pred = predict(rf_classifier, data = thrones_test)$predictions, .after = met_dog_flag)

thrones_test_pred_rf
```

We can see the performance of the random forest and compare the performance toa single decision tree


```{r}
accuracy_rf <- thrones_test_pred_rf %>% 
  accuracy(truth = met_dog_flag, estimate = pred)

accuracy
accuracy_rf
```

```{r}
sensitivity_rf <- thrones_test_pred_rf %>% 
  sensitivity(truth = met_dog_flag, estimate = pred)

sensitivity
sensitivity_rf
```

```{r}
specificity_rf <- thrones_test_pred_rf %>% 
  specificity(truth = met_dog_flag, estimate = pred)

specificity
specificity_rf
```


Considerable increases! RF model much better at correctly predicting characters who did meet a dog.


























































