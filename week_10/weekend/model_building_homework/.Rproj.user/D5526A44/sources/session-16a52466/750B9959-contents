---
title: "Supervised vs Unsupervised learning"
output: html_notebook
---

## Supervided vs unsupervised learning


### Learning objectives

 -   Become aware of the types of problem ML can be used for
 -  Know the difference between supervised and unsupervised algorithms


### 3 types of problems

- Regression
  - supervised
  
- Classification
  - supervised
  
- Clustering
  - unsupervised
  
  
  > What does supervised means?
  
  for each 'x' we know the corresponding 'y'
  
  for each cat picture, we know that it is indeed a cat (or not)!
  
  
  We have a label for each individual.... which we can use to verify the models conclusions
  
  > unsupervised means there are no labels.
  
  These methods shouldn't be thought of in isolation
  
  Maybe:
  
  cluster => identify groups (i.e. labels)
          => regression
  
  
  
  
```{r}
library(tidyverse)
library(fastDummies) # making dummy variables
library(mosaicData)
library(tidyverse)
library(janitor)
library(GGally)
library(ggfortify)
library(mosaic)
```
  
  
```{r}
grades <- read_csv("supervised_vs_unsupervised/data/grades.csv")
```
  ### Learning Objectives

  - Understand what variable engineering is
  - Understand what dummy variables are

   -  how to create and use them

   - Understand the difference between raw and derived variables
   - Know what feature scaling is and how to use it
   
   
  > ‘The features you use influence more than everything else the result. No algorithm alone, to my knowledge, can supplement the information gain given by correct feature engineering‘. - Luca Massaron (Data Scientist & author)
  
> ‘Much of the success of machine learning is actually success in engineering features that a learner (MODEL) can understand’ - Scott Locklin (Physicist)

> ? ’Applied machine learning” is basically feature engineering. - Andrew Ng (Computer Scientist)
  
  
```{r}
grades
```
  
  
  
### Missing values

Could drop them - __Meh__

  - rows
  - col

Could impute - better, but be careful

Could ignore them
  
when do we do this...


#### Task 1 - 5 mins
Replace the NA values in our dataset by _imputing_ the mean value of the appropriate columns. Refer to the summary above to see which columns need work.



```{r}
summary(grades)
```

```{r}
grades %>% 
  summarise(across(.col = everything(), .fns = ~sum(is.na(.x))))
```

```{r}
grades %>% 
  mutate(final = coalesce(final, mean(final, na.rm = TRUE)),
         take_home, mean(take_home, na.rm = TRUE))  
  
grades <- grades %>% 
  mutate(across(where(is.numeric), ~ coalesce(., mean(., na.rm = TRUE))))

grades
```

```{r}
skimr::skim(grades)
```

## Outliers

__BE CAREFUL__ what you consider an outlier!!!


Let's not say "outlier"

Let's say __extreme values__


## transformations

For some models we may need or _data_ to follow a normal distribution and real data may not be normal!

There is no assumption that data must be normal (neither predictors or response).

What __should be normal__ is the models errors.

Very common transformations

- log
- sqrt


## Dummy variables

something we do with categorical data

Most models don't work with _words_


__Nominal__ data

bio = 1
eng = 2
fre = 3
math = 4
phys = 5


__Ordinal__ data

low = 1
med = 2
heigh = 3


```{r}
grades %>% 
  #select(subject) %>% # for demo purposes
  mutate(subject_bio = as.integer(subject == 'biology'),
         subject_eng = as.integer(subject == 'english'),
         subject_fre = as.integer(subject == 'french'),
         subject_math = as.integer(subject == 'maths'),
         subject_phy = as.integer(subject == 'physics')
         ) %>% 
  select(-subject_bio, -subject)
```

If we leave all the __dummy__ columns in the data set, we have a redundant column.

This is important because it will lead to a problem called __multicolinearity__.


This is called the __dummy variable trap__

The take home is: 
If we have n categories, we need (n-1) columns.

In ML often called one-hot encoding.

Don't need to do this manually...
(Don't actually need to do it all... unless)

Let's do it with a package

```{r}
grades_subject_dummy2 <- grades %>% 
  fastDummies::dummy_cols(select_columns = 'subject', 
                          remove_selected_columns = TRUE, # we remove the 'subject' column!
                          remove_first_dummy = TRUE) # remove the column 'subject_biology'
```

Could combine more than one level into a single category


categorical variable

Infants.      10
Kids          14
Tweens.       36
Teens.        40
Adults.       46


## Binning 

Turning a continuous variable into a discrete variable
Discretises(?) a continuous variable
EG 

>= 70 = A
>= 60 = B
>= 50 = C
< 50 = FAIL

```{r}
grades_subject_dummy2 %>% 
  mutate(final_grade = case_when(
    final < 50 ~ "FAIL",
    final < 60 ~ "C",
    final < 70 ~ "B",
    final >= 70 ~ "A"
  ), .after = final)
```




```{r}
grades %>% 
  mutate(final_grade = case_when(final >= 70 ~ "A",
                                 final >= 60 ~ "B",
                                 final >= 50 ~ "C",
                                 final < 50 ~ "F",
                                 )) %>% 
  fastDummies::dummy_cols(select_columns = "final_grade",
                          remove_selected_columns = TRUE,
                         remove_first_dummy = FALSE
                          ) %>% 
  select(-final_grade_F)
```



```{r}
grades2 <- grades %>% 
  mutate(subject2 = as.factor(subject), .after = subject,
         subject3 = relevel(subject2, 'english'))

lm4 <- lm(final ~ assignment + subject2, grades2)
lm5 <- lm(final ~ assignment + subject3, grades2)
summary(lm4)
summary(lm5)

fastDummies::dummy_columns(grades, remove_first_dummy = FALSE)
```





## Raw vs derived variables

Derived variables derives from row variables.

## Variable scaling

- Standardisation

```{r}
CodeClanData::tyrell_corp_jobs %>% 
  ggplot(aes(Salary, fill = Position))+
  geom_histogram(position = "dodge")
```

There's 2 step

1. shift the distribution (e.g. shift in such a way that they have both the same center)

2. Rescale
 - put them on a common scale (e.g. common unit)
 
- Standardisation
 - shift them to be centred over 0
 - make the zero point represent the mean
 - think in terms of SD...(We divide by the SD)
 - our "unit" is SD
 
```{r}
CodeClanData::tyrell_corp_jobs %>% 
  mutate(salary = scale(Salary), .by = Position) #calculate the SD with respect of the Position
```


```{r}
assignment_mean <- mean(grades$assignment)
assignment_sd <- sd(grades$assignment)
```

```{r}
ass_sc <- grades %>% 
  select(assignment) %>% 
  mutate(assignment_scaled = (assignment - assignment_mean)/assignment_sd)
ass_sc
```


```{r}
ass_sc %>% 
  ggplot(aes(assignment))+
  geom_density()+
  geom_vline(xintercept = assignment_mean, size = 1, colour = 2)+
  labs(title = "Raw data")



ass_sc %>% 
  ggplot(aes(assignment_scaled))+
  geom_density()+
  geom_vline(xintercept = mean(ass_sc$assignment_scaled), size = 1, colour = 2)+
  labs(title = "Raw data")
```


```{r}
grades %>% 
  select(assignment) %>% 
  mutate(sc_ass_TT = scale(assignment),
         sc_ass_FT = scale(assignment, scale = F, center = T),
         sc_ass_TF = scale(assignment, scale = T, center = F))
```

# Multiple linear regression


### Learning Objectives



 -    Understand and be able to _build upon_ simple linear regression by _adding predictors_ (continuous and categorical).
 -    Understand and be able to use _forumla notation_

      -   the +, * and : operators
      -   (they mean something different in a formula)

  -   Have _seen_ a _parallel slopes_ model
  -   Be able to _add interactions_ between predictors and _interpret their effects_
  -   Understand that multiple continuous predictors lead to _‘planes of best fit’_
  -   Be able to use _conditional plots_ to _investigate interactions_ between continuous predictors


Multiple linear regression

  - could be continuous
  - could be categorical
  - could be ..........
  
  
```{r}
# comes from mosaic Data package
RailTrail
```
  
  
## Variable engineering
  
```{r}
railtrail_clean <- RailTrail %>%
  clean_names() %>%
  mutate(across(spring:fall, as.logical))

railtrail_clean
```

```{r}
railtrail_trim <- railtrail_clean %>% 
  select(-c(hightemp, lowtemp, fall, day_type))
```
  
  
```{r}
mod1 <- lm(volume ~ ., railtrail_clean)

alias(mod1)
```

```{r}
# GGally
ggpairs(railtrail_trim, progress = FALSE)
```

  
```{r}
railtrail_trim %>% 
  ggplot(aes(x = avgtemp, y = volume))+
  geom_point()+
  geom_smooth(method = 'lm') #method = 'lm', se = FALSE if you don't want  grey strip
```

  
```{r}
mod1 <- lm(volume ~ avgtemp, railtrail_trim)

autoplot(mod1)+ 
  theme_minimal()

  hist(mod1$residuals)

```
  
  

  
 

----
  
the p-val is the probability of getting a value _as extreme_ or _more_ extreme than we observed _IF THE NULL WAS TRUE_.


The observed value depends on the _DATA_

If the data are weird....
...they don't conform with the NULL hypothesis

> How does _H0_ relate to the p-val
> What does the NULL mean with respect to p


> What is the null hypothesis?

- how we expect the world to be

> How do we test hypothesis?

We collect some data and we see if it fits our belief of how the world is
Specifically how the null hypothesis world is

> How do we measure the conformity of the data compared with the NULL world?

That's what the p-value is for.

> How does the p-value helps with this?

the p-value measures how usual our data are compared to the NULL world

Or...

How well the observed stat we take from the data, conforms to our NULL stat

> Yeah, but how?

A very small p-value would mean that there was a little chance of getting those data _if the NULL was true_.
Or...
The chance of getting such an extreme observed stat, was unlikely

A large p-value, suggest the data are consistent with the NULL-world


> But it's just a threshold?

yes!

----

From the first plot we can assume that this is not alinear modul.
  
```{r}
summary(mod1)

fivenum(mod1$residuals)
```


 What does the _Estimate_ column tell us?
  - Tell point estimate
  - Gradient of the line
  - every unit increase in x = 4.8 in y
  - every extra degree in temp = an extra 5 trail users
  - Effect size <=> estimate
  - _H0:_ => emp has no effect 
  
'If' the _null hypothesis_ was true, we would expect a slope of around 0
Our slope is actually ~4.8 - so it is different from H0
But is it really?






```{r}
sd(mod1$residuals)
railtrail_trim %>% 
  ggplot(aes(y = volume))+
  geom_boxplot()
```



## Step-wise regression

1. Start with a NULL model
  - a model with no predictors

2. Add the predictor that gives the lowest p-value
  - we had to test each and everyone - one a time
  
3. examine the residuals again

4. Repeat step 2

5. Repeat step 3



## Parallel slopes model

H0: 'weekday' has no effect on volume
H1: Being a weekend matters


Task: Try plotting an appropriate visualisation to determine whether user volume is associated with the weekday predictor.

```{r}
railtrail_trim %>% 
  ggplot(aes(x = weekday, y = volume)) +
  geom_boxplot()
```


```{r}
summarise(railtrail_trim, cor = cor(weekday, volume))
```

Weak negative correlation!



"Formula language"
 - aka "patsy"
 - Wilkinson notation (also wilkinson-Rodgers)

```{r}
mod2 <- lm(volume ~ avgtemp + weekday, railtrail_trim)

autoplot(mod1) + theme_minimal()
autoplot(mod2) + theme_minimal()
```



```{r}
summary(mod2)
```

The reason we remove one of our dummies is...
The one that is removed becomes our reference class.

```{r}
summary(mod2)

coefficients(mod2)

# mosaic package
plotModel(mod2)
```


163 - 70 the other interception

The dummy encoding thing removes the variables that comes first in the alphabet


Which of these would you prefer to be you reference class

Low
Medium
High


Task: Try adding the summer categorical predictor to the existing model with avgtemp and weekday.


```{r}
mod3 <- lm(volume ~ avgtemp + weekday + summer, railtrail_trim)

autoplot(mod3)
```
    How many lines do you expect to see in this model?
    Is this a parallel slopes model? [Hint try plotModel() on the model object]
    Is the addition of this predictor justified [Hint what is the p-value of summer]?


```{r}
plotModel(mod3)
```



```{r}
summary(mod3)
```

weekday false come first in the alphabet, so it is our intercept



```{r}
pnorm
qnorm
pt(3.898, 86, lower.tail = FALSE)*2 # 86 degrees of freedom
```


```{r}
mod1

ggplot(railtrail_trim, aes(avgtemp, volume))+
  geom_point()+
  geom_smooth(method = 'lm', se = FALSE)+
  geom_point(x = mean(railtrail_trim$avgtemp),
                      y = mean(railtrail_trim$volume),
             colour = 2, size = 4)+
  geom_point(x = 50, y = 99.602 + 4.802 * 50,
             colour = 4, size = 4)
```



```{r}
ggplot(railtrail_trim, aes(avgtemp, volume, colour = weekday))+
  geom_point()+
  geom_smooth(method = 'lm', se = FALSE)+
  geom_point(x = mean(railtrail_trim$avgtemp),
                      y = mean(railtrail_trim$volume),
             colour = 2, size = 4)+
  geom_point(x = 50, y = 99.602 + 4.802 * 50,
             colour = 4, size = 4)
```

```{r}
mod2

#Call:
#lm(formula = volume ~ avgtemp + weekday, data = railtrail_trim)

#Coefficients:
#(Intercept)      avgtemp  weekdayTRUE  
#    163.022        4.541      -70.320  


if x = 40
then
y = bx
  = 163 + 4.5 * 40


ggplot(railtrail_trim, aes(avgtemp, volume, colour = weekday))+
  geom_point()+
  geom_abline(intercept = 163.022, slope = 4.541, colour = 2)+
  geom_abline(intercept =  -70.320, slope = 4.541, colour = 4)+
  #geom_smooth(method = 'lm', se = FALSE)+
  geom_point(x = mean(railtrail_trim$avgtemp),
                      y = mean(railtrail_trim$volume),
             colour = "grey", size = 4)+
  geom_point(x = 50, y = 99.602 + 4.802 * 50,
             colour = "grey", size = 4)


```

```{r}
# if x = 40
# then
# y = a + bx + b2 * x
#   = 163 + 4.5 * 40 + (-70.320) * 0
#   = 163 + 4.5 * 40 + (-70.320) * 1

ggplot(railtrail_trim, aes(avgtemp, volume, colour = weekday)) +
  geom_point() +
  # geom_smooth(method = lm, se = FALSE) +
  geom_abline(intercept = 163.022, slope = 4.541, colour = 2) +
  geom_abline(intercept = 163.022 -70.320, slope = 4.541, colour = 4) +
  geom_point(x = mean(railtrail_trim$avgtemp),
             y = mean(railtrail_trim$volume),
             colour = 'grey', size = 4) +
  geom_point(x = 50,
             y = 99.602 + 4.802 * 50,
             colour = 'grey', size = 4) +
  geom_point(x = 50,
             y = 99.602 + 4.802 * 50,
             colour = 'grey', size = 4) +
  geom_point(x = 40,
             y = 163 + 4.5 * 40,
             colour = 2, size = 4) +
  geom_point(x = 40,
             y = 163 + 4.5 * 40 + (-70.320) * 0,
             colour = 2, size = 4) +
  geom_point(x = 40,
             y = 163 + 4.5 * 40 + (-70.320) * 1,
             colour = 2, size = 4)
```


### Interactions


```{r}
railtrail_trim %>%
  ggplot(aes(x = avgtemp, y = volume, color = weekday)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE)
```

Interaction means...

The effect of one variable on the response is different from the different values of some other variables.


So... is some other variable happens to be a category (e.g. 'avgtemp'), then the effect of temperature on the response ('volume') is different for each value of 'weekday'.


Wilkinson-Rogers notation (patsy).

There are 2 (for now) ways to specify an interaction one way uses a ':' between the variables we think interact

```{r}
summary(mod2)
```



```{r}
mod4 <- lm(volume ~ avgtemp + weekday + avgtemp:weekday, railtrail_trim)
mod4 <- lm(volume ~ avgtemp * weekday, railtrail_trim)
summary(mod4)
```


'avgtemp:weekdayTRUE    3.300'

This number is a _slope deviance_
i.e. is the amount by which the slope _deviates_ from the _main_slope of '2.467', (which applies to the reference category.

WHEN THE CATEGORY IS NOT THE REFERENCE!

Each level of the category will have its own slope deviance.

Is it additive or multiplicative


So... it is just one more term in our $a + bx$ equation, which means that the term itself is additivi, which means that is get applied to the data in the same way as the _intercept deviance_

i.e. if the cat is 0, o gets added,
if the cat is 1 then '1 * the interaction value gets added'.






If we have an 'interaction' that is _sig_ but the 'main effect' is _non sig_, can we remove the 'main effect' from the model?

__NO!!__



#### Task

Examine the summary of the model _including_the interaction between 'avgtemp' and 'weekday'.
Is our inclusion of the interaction justified?


```{r}
summary(mod4)
```

```{r}
model.matrix(mod4)
```


## Adding a continuous interaction


```{r}
summary(mod1)

summary(mod2)
summary(mod3)
summary(mod4)

```

Adjusted r^2 can help to decide if a new varaiable is gìbringing anything to the party (but can still be wrong!!!)


## Comparing models

We need a formal test to compare models.

The formal test is called __ANOVA__.
The function is called 'anova()'....but watch out for 'aov()'.


```{r}
summary(mod1)
anova(mod1, mod2)
anova(mod2,mod3)
```
In the first analysis we prefer 'mod2' because RSS is significantly lower.

In the second analysis we still prefer mod 2 because, the difference between RSS is not significant, so we prefer the simplest model!


```{r}
anova(mod2,mod4)
```


## Degrees of freedom


```{r}
summary(mod4)
```


```{r}
dim(railtrail_trim)
```
number of rows (90) is significantly close to the number of degrees of freedom for mod4 (86).

The degrees of freedom are essentially the differences between the number of rows and the estimates you consider in your model. It good to keep this value high because this means that you need less estimates for getting a "line"...


```{r}
railtrail_trim %>% 
  ggplot(aes(cloudcover, volume))+
  geom_point()+
  geom_smooth(method = 'lm', se = FALSE)
```



```{r}
mod6 <- lm(volume ~ avgtemp + weekday + cloudcover, railtrail_trim)
autoplot(mod6)+ theme_minimal()
```

```{r}
summary(mod6)
```


The _fitted coefficient_ of a continuous predictor in a _multiple linear regression_ model equals the :change in response_ variable for a 1 unit increase in that predictor

> with all other predictor variables held constant.


```{r}
mod7 <- lm(volume ~ avgtemp * cloudcover + weekday, railtrail_trim)
summary(mod7)
```


```{r}
coplot(volume ~ avgtemp | cloudcover,  data = railtrail_trim, 
       rows = 1,
       panel = function(x, y, ...){
         points(x , y)
         abline(lm(y ~ x), col = "blue")
       })
```




























































  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  
  














  
  
  
  
  
  
  
  
  
  
  
  
  